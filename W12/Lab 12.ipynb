{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import reuters, imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN, GRU, Dense, Dropout, Activation, Embedding, Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000 train sequences\n",
      "32000 test sequences\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "# load in training/test set\n",
    "data = pd.read_csv('tweets.160k.random.csv', encoding='utf-8')\n",
    "data.head()\n",
    "\n",
    "data['label'].value_counts()\n",
    "\n",
    "vocab_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocab_size)\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "word_index = tokenizer.word_index\n",
    "tweets = sequence.pad_sequences(sequences, padding='post', maxlen=50)\n",
    "\n",
    "\n",
    "labels = data['label']\n",
    "labels = labels.replace(4,1) # replace label '4' with '1' to facilitate one-hot encoding\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2)\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train) # 2 classes\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          6910550   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 100)         10100     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 100)         30100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 160)         64160     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 160)         128160    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 160)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               147968    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 7,291,296\n",
      "Trainable params: 380,746\n",
      "Non-trainable params: 6,910,550\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "800/800 [==============================] - 44s 55ms/step - loss: 0.5703 - accuracy: 0.6985 - val_loss: 0.5193 - val_accuracy: 0.7434\n",
      "Epoch 2/3\n",
      "800/800 [==============================] - 45s 56ms/step - loss: 0.5054 - accuracy: 0.7525 - val_loss: 0.4958 - val_accuracy: 0.7576\n",
      "Epoch 3/3\n",
      "800/800 [==============================] - 45s 56ms/step - loss: 0.4794 - accuracy: 0.7707 - val_loss: 0.4997 - val_accuracy: 0.7604\n",
      "250/250 [==============================] - 4s 17ms/step - loss: 0.5002 - accuracy: 0.7601\n",
      "Test score: 0.5001832842826843\n",
      "Test accuracy: 0.7601249814033508\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False))\n",
    "\n",
    "model.add(Conv1D(100, 2, activation='relu'))\n",
    "model.add(Conv1D(100, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(160, 4, activation='relu'))\n",
    "model.add(Conv1D(160, 5, activation='relu'))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=3, verbose=1, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
